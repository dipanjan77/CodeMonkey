{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langugage Processing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization: Breaking a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'Hello', 'World', 'example', 'of', 'using', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data=\"This is a Hello World example of using NLTK\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization: Splitting a paragraph into individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Dipanjan!', 'Who are you?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data=\"My name is Dipanjan! Who are you?\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n",
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "phrases=sent_tokenize(data)\n",
    "words=word_tokenize(data)\n",
    "\n",
    "print(phrases)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words: Stop words are natural language words which have very little meaning, such as “and”, “the”, “a”, “an”, and similar words. These words do not provide immense value in Natural Language Understanding (NLU) and are hence removed before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n",
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords=set(stopwords.words('english'))\n",
    "words=word_tokenize(data)\n",
    "wordsFiltered=[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming: Its a process of reducing derived words to their word stem or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game<--game\n",
      "game<--gaming\n",
      "game<--gamed\n",
      "game<--games\n",
      "match<--match\n",
      "tournament<--tournament\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "words= [\"game\", \"gaming\", \"gamed\", \"games\", \"match\", \"tournament\"]\n",
    "ps=PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word) + \"<--\" + word)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging:  It simply means labeling words with their appropriate Part-Of-Speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PunktSentenceTokenizer is the abstract class of the default sentence tokenizer, i.e. sent_tokenize(). You could also train the punkt tokenizer with your own corpus of data, by passing it in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the little yellow dog barked at the cat.']\n",
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence= \"\"\"the little yellow dog barked at the cat.\"\"\"\n",
    "tokenizer=PunktSentenceTokenizer()\n",
    "tokenized= tokenizer.tokenize(sentence)\n",
    "\n",
    "print (tokenized)\n",
    "\n",
    "for sent in tokenized:\n",
    "    words=nltk.word_tokenize(sent)\n",
    "    tagged=nltk.pos_tag(words)\n",
    "    \n",
    "    print(tagged)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking - Its a process of extracting phrases from unstructured text. Instead of just simple tokens which may not represent the actual meaning of the text, its advisable to use phrases such as “South Africa” as a single word instead of ‘South’ and ‘Africa’ separate words. Chunking works on top of POS tagging, it uses pos-tags as input and provides chunks as output. Similar to POS tags, there are a standard set of Chunk tags like Noun Phrase(NP), Verb Phrase (VP), etc. Chunking is very important when you want to extract information from text such as Locations, Person Names etc. In NLP called Named Entity Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN)\n",
      "  ./.)\n",
      "(NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "(NP the/DT cat/NN)\n"
     ]
    }
   ],
   "source": [
    "#Define your grammer using regular expression. We will consider Noun Phrase Chunking. The rule states that whenever the chunk finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN) then the Noun Phrase(NP) chunk should be formed.\n",
    "\n",
    "grammer = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} #NP\n",
    "    ''')\n",
    "\n",
    "chunkParser=nltk.RegexpParser(grammer)\n",
    "\n",
    "tree=chunkParser.parse(tagged)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
